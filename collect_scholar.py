import csv
import os
import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# --- ğŸ› ï¸ é…ç½®åŒº ---
SAVE_FILE = 'literature_scholar.csv'  # ä¿å­˜åˆ°è¿™ä¸ªæ–°æ–‡ä»¶
# å…³é”®è¯ï¼šæ›´åŠ åå‘å­¦æœ¯ã€è€ƒå¤ã€åœ°ç†
KEYWORDS = [
  "è”æé“",
  "èœ€é“äº¤é€š",
  "å”ä»£é©¿ä¼ åˆ¶åº¦",
  "å¤ä»£é©¿é“",
  "è´¡è”è¿è¾“",
  "å”ä»£äº¤é€šåˆ¶åº¦",
  "é©¿ç«™åˆ¶åº¦ç ”ç©¶",
  "é‚®é©¿åˆ¶åº¦",
  "å”ä»£ç‰©æµå²",
  "å¤ä»£äº¤é€šå²",
  "å”è¯—ä¸­çš„äº¤é€šæ„è±¡",
  "å”ä»£è¡Œæ—…è¯—",
  "é©¿é“æ–‡åŒ–",
  "èœ€é“æ–‡åŒ–ç ”ç©¶",
  "äº¤é€šä¸æ–‡å­¦"
]

MAX_PAGES = 2  # æ¯ä¸ªè¯æŠ“2é¡µ


def setup_driver():
    print("ğŸ“ å¯åŠ¨å­¦æœ¯é‡‡é›†åŠ©æ‰‹...")
    chrome_options = Options()
    # ç™¾åº¦å­¦æœ¯åçˆ¬æ¯”è¾ƒä¸¥ï¼Œå¿…é¡»ç”¨å¯è§†æ¨¡å¼
    # chrome_options.add_argument("--headless")

    # ä¼ªè£…æˆçœŸäººæµè§ˆå™¨
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36")

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver


def init_csv():
    if not os.path.exists(SAVE_FILE):
        with open(SAVE_FILE, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            # è¡¨å¤´è¦å’Œä¹‹å‰çš„ä¿æŒä¸€è‡´ï¼Œæ–¹ä¾¿ç½‘ç«™è¯»å–
            writer.writerow(['id', 'title', 'author', 'era', 'content', 'type', 'source'])


def main():
    driver = setup_driver()
    init_csv()

    # è®¡ç®—å½“å‰ID (é˜²æ­¢å’Œè¯—æ­Œçš„IDå†²çªï¼Œæˆ‘ä»¬ä» 2000 å¼€å§‹ç¼–å·)
    current_id = 2000
    if os.path.exists(SAVE_FILE):
        with open(SAVE_FILE, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            if len(lines) > 1:
                current_id = 2000 + len(lines)

    print(f"ğŸ“š ç›®æ ‡å…³é”®è¯: {KEYWORDS}")

    for keyword in KEYWORDS:
        print(f"\nğŸ” æ­£åœ¨æ£€ç´¢å­¦æœ¯èµ„æ–™: ã€{keyword}ã€‘")

        for page in range(0, MAX_PAGES):
            # ç™¾åº¦å­¦æœ¯çš„åˆ†é¡µé€»è¾‘ï¼šç¬¬1é¡µæ˜¯0ï¼Œç¬¬2é¡µæ˜¯10ï¼Œç¬¬3é¡µæ˜¯20
            pn = page * 10
            url = f"https://xueshu.baidu.com/s?wd={keyword}&pn={pn}&filter=sc_type%3D%7B1%7D"  # sc_type=1 ä»£è¡¨åªçœ‹æœŸåˆŠ/è®ºæ–‡

            driver.get(url)
            time.sleep(random.uniform(3, 5))  # å¤šä¼‘æ¯ä¸€ä¼šï¼Œå­¦æœ¯ç½‘ç«™æ¯”è¾ƒæ•æ„Ÿ

            try:
                # æ‰¾åˆ°æ‰€æœ‰çš„è®ºæ–‡å¡ç‰‡
                items = driver.find_elements(By.CSS_SELECTOR, ".result")

                if len(items) == 0:
                    print("      âš ï¸ æœ¬é¡µæ— å†…å®¹æˆ–é‡åˆ°éªŒè¯ç ï¼Œè·³è¿‡...")
                    break

                for item in items:
                    try:
                        # 1. æŠ“å–æ ‡é¢˜
                        title_elem = item.find_element(By.CSS_SELECTOR, "h3 a")
                        title = title_elem.text
                        link = title_elem.get_attribute("href")

                        # 2. æŠ“å–æ‘˜è¦ (Content)
                        try:
                            abstract_elem = item.find_element(By.CSS_SELECTOR, ".c_abstract")
                            content = abstract_elem.text.replace("\n", "").replace("æ‘˜è¦ï¼š", "")
                        except:
                            content = "æš‚æ— æ‘˜è¦é¢„è§ˆ..."

                        # 3. æŠ“å–ä½œè€…å’Œå¹´ä»½ (Era)
                        # ç™¾åº¦å­¦æœ¯çš„ä½œè€…ä¿¡æ¯æ¯”è¾ƒæ‚ï¼Œæˆ‘ä»¬ç›´æ¥æŠ“å–ä¸‹æ–¹çš„ä¸€è¡Œå°å­—
                        try:
                            info_elem = item.find_element(By.CSS_SELECTOR, ".sc_info")
                            info_text = info_elem.text
                            # ç®€å•çš„å¹´ä»½æå–é€»è¾‘ï¼šæ‰¾ 19xx æˆ– 20xx
                            import re
                            year_match = re.search(r'(19|20)\d{2}', info_text)
                            era = year_match.group(0) + "å¹´" if year_match else "ç°ä»£"

                            # æå–ä½œè€… (å–ç¬¬ä¸€ä¸ªåå­—)
                            author = info_text.split("-")[0].strip()
                        except:
                            era = "ç°ä»£"
                            author = "å­¦æœ¯ç ”ç©¶ç»„"

                        # 4. å†™å…¥ CSV
                        with open(SAVE_FILE, 'a', newline='', encoding='utf-8') as f:
                            writer = csv.writer(f)
                            # type å›ºå®šä¸º 'å­¦æœ¯ç ”ç©¶'ï¼Œæ–¹ä¾¿å‰ç«¯æ˜¾ç¤ºä¸åŒé¢œè‰²
                            writer.writerow([current_id, title, author, era, content, 'å­¦æœ¯ç ”ç©¶', link])

                        print(f"      âœ… [{current_id}] {title[:20]}...")
                        current_id += 1

                    except Exception as e:
                        continue

            except Exception as e:
                print(f"      âŒ é¡µé¢å‡ºé”™: {e}")

    print(f"\nğŸ‰ å­¦æœ¯é‡‡é›†å®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ° {SAVE_FILE}")
    print("ğŸ’¡ æç¤ºï¼šç™¾åº¦å­¦æœ¯å¦‚æœå¼¹å‡ºéªŒè¯ç ï¼Œè¯·æ‰‹åŠ¨ç‚¹å‡»ä¸€ä¸‹ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨ç»§ç»­ã€‚")
    # driver.quit()


if __name__ == '__main__':
    main()